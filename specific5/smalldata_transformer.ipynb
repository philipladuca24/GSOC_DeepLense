{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smalldata transformer\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import sklearn.metrics as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "label_dict = {0 : \"no_sub\", 1 : \"sub\"}\n",
    "with open('imgs.pkl', 'rb') as f:\n",
    "    imgs = pickle.load(f)\n",
    "with open('labels.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre processing\n",
    "rescale = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(scale=1 / 255)\n",
    "])\n",
    "one_hot_fun = tf.keras.Sequential([\n",
    "    tf.keras.layers.CategoryEncoding(num_tokens=2, output_mode=\"one_hot\")\n",
    "])\n",
    "\n",
    "train_imgs, test_imgs, train_labels, test_labels = train_test_split(imgs, labels, test_size=0.1, shuffle=True)\n",
    "\n",
    "train_imgs = rescale(train_imgs)\n",
    "test_imgs = rescale(test_imgs)\n",
    "train_labels = one_hot_fun(train_labels)\n",
    "test_labels = one_hot_fun(test_labels)\n",
    "\n",
    "def orthogonal_rot(image):\n",
    "    return np.rot90(image, np.random.choice([-1, 0, 1]))\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    preprocessing_function=orthogonal_rot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "BATCH_SIZE = 256\n",
    "IMAGE_SIZE = 150\n",
    "PATCH_SIZE = 15\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "EPOCHS = 50\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "TRANSFORMER_LAYERS = 6\n",
    "PROJECTION_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "TRANSFORMER_UNITS = [\n",
    "    PROJECTION_DIM * 2,\n",
    "    PROJECTION_DIM,\n",
    "]\n",
    "MLP_HEAD_UNITS = [512, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create shifted images and patches\n",
    "class ShiftedPatchTokenization(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_patches=NUM_PATCHES,\n",
    "        projection_dim=PROJECTION_DIM,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.half_patch = patch_size // 2\n",
    "        self.flatten_patches = tf.keras.layers.Reshape((num_patches, -1))\n",
    "        self.projection = tf.keras.layers.Dense(units=projection_dim)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "\n",
    "    def crop_shift_pad(self, images, mode):\n",
    "        if mode == \"left-up\":\n",
    "            crop_height = self.half_patch\n",
    "            crop_width = self.half_patch\n",
    "            shift_height = 0\n",
    "            shift_width = 0\n",
    "        elif mode == \"left-down\":\n",
    "            crop_height = 0\n",
    "            crop_width = self.half_patch\n",
    "            shift_height = self.half_patch\n",
    "            shift_width = 0\n",
    "        elif mode == \"right-up\":\n",
    "            crop_height = self.half_patch\n",
    "            crop_width = 0\n",
    "            shift_height = 0\n",
    "            shift_width = self.half_patch\n",
    "        else:\n",
    "            crop_height = 0\n",
    "            crop_width = 0\n",
    "            shift_height = self.half_patch\n",
    "            shift_width = self.half_patch\n",
    "\n",
    "        crop = tf.image.crop_to_bounding_box(\n",
    "            images,\n",
    "            offset_height=crop_height,\n",
    "            offset_width=crop_width,\n",
    "            target_height=self.image_size - self.half_patch,\n",
    "            target_width=self.image_size - self.half_patch,\n",
    "        )\n",
    "        shift_pad = tf.image.pad_to_bounding_box(\n",
    "            crop,\n",
    "            offset_height=shift_height,\n",
    "            offset_width=shift_width,\n",
    "            target_height=self.image_size,\n",
    "            target_width=self.image_size,\n",
    "        )\n",
    "        return shift_pad\n",
    "\n",
    "    def call(self, images):\n",
    "        images = tf.concat(\n",
    "            [\n",
    "                images,\n",
    "                self.crop_shift_pad(images, mode=\"left-up\"),\n",
    "                self.crop_shift_pad(images, mode=\"left-down\"),\n",
    "                self.crop_shift_pad(images, mode=\"right-up\"),\n",
    "                self.crop_shift_pad(images, mode=\"right-down\"),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        flat_patches = self.flatten_patches(patches)\n",
    "        tokens = self.layer_norm(flat_patches)\n",
    "        tokens = self.projection(tokens)\n",
    "        return (tokens, patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting data\n",
    "# image = train_imgs[np.random.choice(range(train_imgs.shape[0]))]\n",
    "# resized_image = tf.image.resize(\n",
    "#     tf.convert_to_tensor([image]), size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    "# )\n",
    "\n",
    "# (token, patch) = ShiftedPatchTokenization()(resized_image)\n",
    "# (token, patch) = (token[0], patch[0])\n",
    "# n = patch.shape[0]\n",
    "# shifted_images = [\"ORIGINAL\", \"LEFT-UP\", \"LEFT-DOWN\", \"RIGHT-UP\", \"RIGHT-DOWN\"]\n",
    "# for index, name in enumerate(shifted_images):\n",
    "#     print(name)\n",
    "#     count = 1\n",
    "#     plt.figure(figsize=(4, 4))\n",
    "#     for row in range(n):\n",
    "#         for col in range(n):\n",
    "#             plt.subplot(n, n, count)\n",
    "#             count = count + 1\n",
    "#             image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 5 * 1))\n",
    "#             plt.imshow(image[..., index : index + 1], vmin=0, vmax=1)\n",
    "#             plt.axis(\"off\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "class PatchEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "\n",
    "    def call(self, encoded_patches):\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_patches = encoded_patches + encoded_positions\n",
    "        return encoded_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention\n",
    "class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # The trainable temperature term. The initial value is\n",
    "        # the square root of the key dimension.\n",
    "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n",
    "\n",
    "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
    "        query = tf.multiply(query, 1.0 / self.tau)\n",
    "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "        attention_scores_dropout = self._dropout_layer(\n",
    "            attention_scores, training=training\n",
    "        )\n",
    "        attention_output = tf.einsum(\n",
    "            self._combine_equation, attention_scores_dropout, value\n",
    "        )\n",
    "        return attention_output, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp and attention mask\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = tf.keras.layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
    "diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vit model\n",
    "def create_vit_classifier():\n",
    "    inputs = tf.keras.layers.Input(shape=(150,150,1))\n",
    "    # Create patches.\n",
    "    (tokens, _) = ShiftedPatchTokenization()(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder()(tokens)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(TRANSFORMER_LAYERS):\n",
    "        # Layer normalization 1.\n",
    "        x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = MultiHeadAttentionLSA(\n",
    "            num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "        )(x1, x1, attention_mask=diag_attn_mask)\n",
    "        # Skip connection 1.\n",
    "        x2 = tf.keras.layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = tf.keras.layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = tf.keras.layers.Flatten()(representation)\n",
    "    representation = tf.keras.layers.Dropout(0.2)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.2)\n",
    "    # Classify outputs.\n",
    "    logits = tf.keras.layers.Dense(2)(features)\n",
    "    logits = tf.keras.layers.Activation('softmax')(logits)\n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "    model.compile(\n",
    "    optimizer= tfa.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_vit_classifier()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train moodel\n",
    "model.fit(\n",
    "    datagen.flow(train_imgs, train_labels, batch_size=BATCH_SIZE),\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "model.save('lowdata_trans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions and eval\n",
    "pred_label = model.predict(test_imgs)\n",
    "eval = model.evaluate(test_imgs,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc, auc and plotting\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (8, 8))\n",
    "\n",
    "fpr , tpr , thresholds = skl.roc_curve (np.array(test_labels[:,0]), np.array(pred_label[:,0]))\n",
    "c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (label_dict[0], skl.auc(fpr, tpr)))\n",
    "c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "print('ROC AUC score:', skl.roc_auc_score(np.array(test_labels), np.array(pred_label)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d198aacd62101d4c7e30f088a8298ef86ef907cfe0ab142190ab65cf39c3b802"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
